# Bi-Weekly Report 3
**Name**: Samhita Kolluri   
**Date Range**: February 28 - March 14, 2025

## Summary of Work Completed
- Developed `llm_handler.py` to generate four semantically contrastive documents per input using prompt engineering techniques.
- Implemented `vectorizer.py` to compute dual embeddings using `mixedbread-ai/mxbai-embed-large-v1`, producing:
  - **1024D topic embeddings** for semantic similarity.
  - **1024D stance embeddings** (currently mirrored until the stance model is integrated).
  - Resulting **2048D combined embeddings** for downstream scoring and retrieval.
- Integrated **ChromaDB** for persistent multi-vector document storage, enabling fast approximate nearest neighbor (ANN) search on topic and stance vectors.
- Logged and validated vector shapes  and storage IDs. Verified distance metrics for retrieval sanity checks.

## Challenges Encountered

**Challenge 1:**  
High variance and low contrast quality in initial LLM outputs due to overly generic prompts.

**Solution 1:**  
Introduced structured prompt templates and N-shot learning examples to guide the LLM toward generating well-separated, contrastive outputs. Controlled the type of opposition using conditional instruction patterns.


## Goals for the Next Two Weeks
- Build contrastive scoring logic in `contrastive_score.py`.
- Implement hybrid scoring and retrieval ranking logic.
- Add test scripts to validate retrieval quality.
- Develop `contrastive_score.py` to compute hybrid contrastive relevance scores using:
  - Dot product or cosine similarity for topic relevance.
  - Weighted scoring formula:  `final_score = topic_sim * (1 + stance_opp)`.
- Implement `retrieve_top_docs.py` to extract and rank top-k contrastive candidates.
- Build test scripts to validate pipeline performance, especially ranking consistency and vector retrieval precision.
- Begin integrating stance model (fine-tuned stance model if available) into `vectorizer.py`.

## Hours Contributed During this Period
40 hours
