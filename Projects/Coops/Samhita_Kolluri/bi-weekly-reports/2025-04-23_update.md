# Bi-Weekly Report 6
**Name**: Samhita Kolluri   
**Date Range**: April 16 - April 23, 2025

## Summary of Work Completed
- Successfully fine-tuned a stance-aware language model based on `meta-llama/Llama-3.2-3B-Instruct`, using a curated SNLI-style contradiction dataset focused on premise–hypothesis semantic opposition.
- Uploaded the final model to Hugging Face with metadata, configuration, and versioning:  
  [`Samhita-kolluri/llama-contrastive-module-stance`](https://huggingface.co/Samhita-kolluri/llama-contrastive-module-stance)
- Integrated the model into the contrastive scoring pipeline, updating `vectorizer.py` to generate improved stance embeddings, and replacing placeholder scoring logic in `contrastive_score.py`.
- Validated the model’s impact on scoring quality by comparing pre- and post-integration results across a standardized set of test inputs. Observed an increased spread in contrastive scores and more sharply differentiated semantic oppositions.


## Challenges Encountered

**Challenge 1:**  
Training instability due to gradient oscillations and slow convergence during fine-tuning.

**Solution 1:**  
Stabilized training using a tuned learning rate schedule, gradient clipping, and optimizer warm-up. Introduced mixed-precision training for memory efficiency.

**Challenge 2:**  
Hardware limitations (GPU VRAM exhaustion) during long training epochs.

**Solution 2:**  
Implemented small batch sizes with gradient accumulation and checkpointing. Enabled automatic resume from intermediate checkpoints in the event of resource interruptions.

## Goals for the Next Two Weeks
- Complete and polish all final deliverables:
  - `Final_Report.md`, updated `README.md`, and system design diagrams.
- Prepare optional handover materials including architecture visualizations and a demo walkthrough.
- Publish a technical Medium article summarizing the approach and results.

## Hours Contributed During this Period
20 hours
